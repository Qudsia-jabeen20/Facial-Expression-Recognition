{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/D1nnZko35xX3dW9cBAEc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Qudsia-jabeen20/Facial-Expression-Recognition/blob/main/emotion_detection_with_facial_expression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "import warnings\n",
        "if not sys.warnoptions:\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "uoir9VzSU5AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ananthu017/emotion-detection-fer\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "ZTZMY_-rfA2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"Files in dataset directory:\")\n",
        "print(os.listdir(path))\n"
      ],
      "metadata": {
        "id": "B_AQ089UflAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "img_size = (48, 48)\n",
        "batch_size = 32\n",
        "\n",
        "train_dir = os.path.join(path, 'train')\n",
        "test_dir = os.path.join(path, 'test')\n",
        "\n",
        "train_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n",
        "\n",
        "test_gen = ImageDataGenerator(rescale=1./255).flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    color_mode='grayscale'\n",
        ")\n"
      ],
      "metadata": {
        "id": "FJHmTkbPfneZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Conv2D(64, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(train_gen.num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "uS0lxL38f6jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_gen, validation_data=test_gen, epochs=10)\n"
      ],
      "metadata": {
        "id": "_myQGrDSgR4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = model.evaluate(test_gen)\n",
        "print(f\"Test Accuracy: {acc:.2f}\")\n"
      ],
      "metadata": {
        "id": "qCKg81FTlhpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_gen)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Hoh3hcR1lrBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    img = image.load_img(filename, color_mode='grayscale', target_size=(48, 48))\n",
        "    img_array = image.img_to_array(img) / 255.0\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    pred = model.predict(img_array)\n",
        "    predicted_label = np.argmax(pred)\n",
        "    print(\"Predicted Emotion:\", class_labels[predicted_label])\n"
      ],
      "metadata": {
        "id": "TdZgaMV6m6n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.saving.save_model(model, 'my_model.keras')\n"
      ],
      "metadata": {
        "id": "2mafQn_coAFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet"
      ],
      "metadata": {
        "id": "1nlXNT-5nnpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from tensorflow.keras.models import load_model\n",
        "import cv2\n",
        "\n",
        "\n",
        "model = load_model(\"emotion_model.h5\")\n",
        "emotion_labels = ['angry', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
        "\n",
        "\n",
        "def predict_emotion(pil_img):\n",
        "    try:\n",
        "        img = np.array(pil_img)\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "        face = cv2.resize(gray, (48, 48))\n",
        "        face = face.astype(\"float32\") / 255.0\n",
        "        face = np.expand_dims(face, axis=-1)\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "\n",
        "        prediction = model.predict(face)[0]\n",
        "        top_emotions = {emotion_labels[i]: float(prediction[i]) for i in range(len(prediction))}\n",
        "        return top_emotions\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error: {e}\"\n",
        "        gr.Interface(\n",
        "    fn=predict_emotion,\n",
        "    inputs=gr.Image(source=\"webcam\", tool=None),\n",
        "    outputs=gr.Label(num_top_classes=3),\n",
        "    live=True,\n",
        "    title=\"Real-Time Emotion Detection\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "id": "fmK3HNRJs7CR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}